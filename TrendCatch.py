from konlpy.tag import Okt
import requests
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import random
from difflib import SequenceMatcher

# Step 1: ë„¤ì´ë²„ API ë°ì´í„° ìˆ˜ì§‘
def fetch_naver_data(query, api_url, client_id, client_secret, display=10):
    """ë„¤ì´ë²„ APIë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘"""
    headers = {
        "X-Naver-Client-Id": "3K8lZhOl84EithJ3EWIb",
        "X-Naver-Client-Secret": "80yNaLWC1m"
    }
    params = {
        "query": query,
        "display": display,
        "start": 1
    }

    try:
        response = requests.get(api_url, headers=headers, params=params)
        response.raise_for_status()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = response.json()
        return [item['title'] + " " + item['description'] for item in data['items']]
    except Exception as e:
        print(f"Error fetching data from Naver API: {e}")
        return []

# Step 2: ë°ì´í„° ì „ì²˜ë¦¬
def preprocess_text(texts):
    """ê°„ë‹¨í•œ ì „ì²˜ë¦¬: íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜"""
    processed_texts = []
    for text in texts:
        text = re.sub(r"[^\w\sê°€-í£]", "", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        processed_texts.append(text)
    return processed_texts

# Step 3: í…ìŠ¤íŠ¸ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë¶„ë¦¬
def split_by_whitespace(texts):
    """ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    result_list = []
    for text in texts:
        separated_words = text.split()
        result_list.extend(separated_words)
    return result_list

# Step 4: TF-IDF ê¸°ë°˜ ì‹ ì¡°ì–´ íƒì§€
def find_neologisms_tfidf(texts, min_tfidf_score=0.1):
    """TF-IDFë¡œ ì‹ ì¡°ì–´ í›„ë³´ë¥¼ íƒì§€"""
    vectorizer = TfidfVectorizer(
        min_df=2,  # ìµœì†Œ ë¹ˆë„: 2ë¡œ ì„¤ì •
        max_df=0.85,  # ìµœëŒ€ ë¹ˆë„: ë¬¸ì„œì˜ 85% ì´í•˜
        token_pattern=r'\b[ê°€-í£]+\b'  # í•œê¸€ ë‹¨ì–´ë§Œ ì¶”ì¶œ
    )
    try:
        tfidf_matrix = vectorizer.fit_transform(texts)
        words = vectorizer.get_feature_names_out()
        tfidf_scores = tfidf_matrix.sum(axis=0).A1

        word_scores = [(word, score) for word, score in zip(words, tfidf_scores) if score >= min_tfidf_score]
        return [word for word, score in sorted(word_scores, key=lambda x: x[1], reverse=True)]
    except ValueError as e:
        print("TF-IDF Error:", e)
        return []  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# Step 5: ì‹ ì¡°ì–´ í•„í„°ë§
def filter_neologisms_advanced(tfidf_candidates):
    okt = Okt()

    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    stopwords = [
        "ëœ»", "ë¶€ë™ì‚°", "ì˜ì–´", "ìš”ì¦˜", "ê·¸", "ëŒ€í•´", "ì•„íŒŒíŠ¸", "ê´€ë ¨", "ê·¸ë¦¬ê³ ", 
        "ëœ»ì—", "ì‹¤ì œ", "ì˜¤ëŠ˜ì€", "ìœ ë˜", "ìˆìŠµë‹ˆë‹¤", "ìì£¼", "ìµœê·¼", "ê³µì—°", 
        "êµ­íšŒ", "ë‹¨ì–´", "ë§ì´", "ì‚¬ë¡€", "ì‚¬ìš©", "ì‚¬ì´ì—ì„œ", "ì•ˆë…•í•˜ì„¸ìš”", "ì˜ì–´ë¡œ", 
        "ìš´ë™ì„", "ìœ ì‚¬í•œ", "ìœ í–‰í•˜ëŠ”", "ì˜ë¯¸ë¥¼", "ì˜ë¯¸í•˜ëŠ”", "ì •ì‹ ì ", "ì •í™•íˆ", 
        "í•„ìš”í•œ", "í•©ë‹ˆë‹¤", "í•©ì¹œ", "í”íˆ", "ìš©ì–´", "ì¸ê¸°", "ì¡°êµ­", "ì²­ì•½", "ê°€ê²©"
    ]

    filtered_words = []
    for word in tfidf_candidates:
        # ë¶ˆìš©ì–´ ì œê±°
        if word in stopwords:
            continue

        # í’ˆì‚¬ ë¶„ì„: ëª…ì‚¬ë§Œ í¬í•¨
        pos_tags = okt.pos(word)
        if not all(tag == "Noun" for _, tag in pos_tags):
            continue

        # ì‹ ì¡°ì–´ íŒ¨í„´ í™•ì¸
        if len(word) >= 3:
            filtered_words.append(word)

    return filtered_words

# ì‹ ì¡°ì–´ ê²€ìƒ‰ ë° ìºì‹±
def search_neologism_meaning(word):
    """ì‹ ì¡°ì–´ì˜ ì˜ë¯¸ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë°˜í™˜"""
    client_id = "3K8lZhOl84EithJ3EWIb"
    client_secret = "80yNaLWC1m"
    url = "https://openapi.naver.com/v1/search/encyc.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {"query": word, "display": 1}

    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()
        print("API Response:", data)  # ì‘ë‹µ ë°ì´í„° í™•ì¸
        if "items" in data and len(data["items"]) > 0:
            meaning = data["items"][0]["description"]
            return meaning.strip()
        else:
            return "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
    except Exception as e:
        return f"ì—ëŸ¬ ë°œìƒ: {e}"

neologism_cache = {}

def get_neologism_meaning(word):
    """ì‹ ì¡°ì–´ì˜ ì˜ë¯¸ë¥¼ ìºì‹±í•˜ì—¬ ë°˜í™˜"""
    if word in neologism_cache:
        return neologism_cache[word]

    meaning = search_neologism_meaning(word)
    neologism_cache[word] = meaning  # ìºì‹±
    return meaning


def is_correct_answer(user_answer, correct_answer, threshold=0.6):
    """ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë‹µ ì¸ì •"""
    similarity = SequenceMatcher(None, user_answer, correct_answer).ratio()
    percentage = similarity * 100
    print(f"ì •ë‹µ ìœ ì‚¬ì„± : {percentage:.1f}%")
    return similarity >= threshold

# ë¬¸í•´ë ¥ í…ŒìŠ¤íŠ¸
def test_neologism_knowledge_with_dynamic_search(neologisms):
    """ë™ì ìœ¼ë¡œ ì‹ ì¡°ì–´ì˜ ì˜ë¯¸ë¥¼ ê²€ìƒ‰í•˜ê³  ë¬¸í•´ë ¥ í…ŒìŠ¤íŠ¸ ì§„í–‰"""
    print("ì‹ ì¡°ì–´ ë¬¸í•´ë ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    score = 0
    total_questions = 3  # ì§ˆë¬¸ ê°œìˆ˜

    for i in range(total_questions):
        word = random.choice(neologisms)
        meaning = get_neologism_meaning(word)

        if meaning == "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ" or "ì—ëŸ¬ ë°œìƒ" in meaning:
            print(f"\në¬¸ì œ {i + 1}: '{word}'ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì œë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            continue

        print(f"\në¬¸ì œ {i + 1}: '{word}'ì˜ ëœ»ì€ ë¬´ì—‡ì¼ê¹Œìš”?")
        user_answer = input("ì •ë‹µì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

        if is_correct_answer(user_answer, meaning):
            print("ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰")
            score += 1
        else:
            print(f"ì˜¤ë‹µì…ë‹ˆë‹¤. ì •ë‹µì€: '{meaning}'ì…ë‹ˆë‹¤.")

    print(f"\ní…ŒìŠ¤íŠ¸ ì¢…ë£Œ! ì´ ì ìˆ˜: {score}/{total_questions}")


# Main íŒŒì´í”„ë¼ì¸
def main():
    # ë„¤ì´ë²„ API ì„¤ì •
    client_id = "3K8lZhOl84EithJ3EWIb"  # ë„¤ì´ë²„ Client ID
    client_secret = "80yNaLWC1m"  # ë„¤ì´ë²„ Client Secret

    # API URL ì„¤ì •
    blog_api_url = "https://openapi.naver.com/v1/search/blog.json"
    news_api_url = "https://openapi.naver.com/v1/search/news.json"

    # ë°ì´í„° ìˆ˜ì§‘
    query = "ì‹ ì¡°ì–´"  # ê²€ìƒ‰ í‚¤ì›Œë“œ
    blog_data = fetch_naver_data(query, blog_api_url, client_id, client_secret, display=10)
    news_data = fetch_naver_data(query, news_api_url, client_id, client_secret, display=10)

    # Step 1: ë°ì´í„° ì¤€ë¹„
    raw_texts = blog_data + news_data

    # Step 2: ì „ì²˜ë¦¬
    processed_texts = preprocess_text(raw_texts)

    # Step 3: ëª…ì‚¬ ì¶”ì¶œ
    words = split_by_whitespace(processed_texts)
    print("split_by_whitespace:", words)
    print("-" * 80)

    # Step 4: TF-IDFë¡œ ì‹ ì¡°ì–´ í›„ë³´ íƒì§€
    tfidf_candidates = find_neologisms_tfidf(words)
    print("TF-IDF Based Candidates:", tfidf_candidates)
    print("-" * 80)

    filtered_neologisms = filter_neologisms_advanced(tfidf_candidates)
    print("Filtered Neologisms:", filtered_neologisms)
    print("-" * 80)

    # ë¬¸í•´ë ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_neologism_knowledge_with_dynamic_search(filtered_neologisms)

if __name__ == "__main__":
    main()

